## 개요
- 모델의 상세사항을 많은 경우에는 실제로 알아야 할 필요는 없음. 
- 그러나 어떻게 작동하는지 이해하고 있으면, 적절한 모델, 올바른 훈련 알고리즘, 작업에 맞는 좋은 하이퍼 파라미터를 빠르게 찾을 수 있음. 또한 디버깅이나 에러를 효율적으로 분석하는데 도움이 됨.

## 4.1 선형회귀
- 선형 모델은 일반적으로 입력 특성의 가중치 합과 편향을 더해 예측을 만듬.

![20220113_163115](https://user-images.githubusercontent.com/75361137/149285483-5499b63a-7bb0-4b35-86db-9e76a943c5de.png)
- y는 예측값이고, n은 특성의 수, x는 특성값, θ는 모델의 파라미터를 의미함.
- 벡터 형태로 표현한다면 아래 처럼

![20220113_163457](https://user-images.githubusercontent.com/75361137/149285785-6d9af4dd-2cdb-40f9-a69e-058380b6aa87.png)
- θ는 특성 가중치를 담은 파라미터 벡터, X는 특성벡터

- 훈련을 시킨다는 것은 가장 잘 맞도록 모델 파라미터를 설정하는 것. 측정 방법은 주로 RMSE(평균 제곱근 오차)를 사용. -> MSE(평균 제곱 오차)를 최소화하는 것이 같은 결과를 내면서 더 간단함.

![20220113_164318](https://user-images.githubusercontent.com/75361137/149286971-180990e7-121d-493c-a27b-713a8a7b1524.png)

### 4.1.1 정규방정식
- MSE 비용 함수를 최소화하는 θ값을 찾기 위한 공식.

![20220113_164807](https://user-images.githubusercontent.com/75361137/149287618-5349343b-d880-4441-9960-7c5b05656891.png)
#### 유사역행렬과 특이값 분해
- 유사역행렬 어떤 m x n 실수 행렬 A에 대해서 다음 4가지 조건을 만족하는 행렬이 무어-펜로즈(Moore-Penrose) 유사 역행렬.
![20220113_171632](https://user-images.githubusercontent.com/75361137/149291828-66b87c22-d3c9-4858-bc41-9753cc094574.png)
- 특이값 분해(SVD)를 이용하면 유사역행렬을 쉽게 계산할 수 있음.
- SVD는 어떤 실수 행렬 A를 3개의 행렬의 곱으로 분해한 것.

![20220113_171840](https://user-images.githubusercontent.com/75361137/149292125-829c704d-6ff2-41f6-abe1-c921da1f3532.png)
- U는 m x m 직각행렬이고, V는 n x n 직각행렬.
- SVD를 이용하면 어떤 m x n 실수 행렬 A의 유사역행렬은 ![image](https://user-images.githubusercontent.com/75361137/149292535-90ee8cb0-2659-448b-b554-4a20c41d917b.png)
- 이 방식이 정규방정식을 구하는 것 보다 훨씬 효율적인 방식.

### 4.1.2 계산 복잡도
- Scikit-learn의 LinearRegression이 사용하는 SVD방법의 계산 복잡도는 O(n^2). 특성의 개수가 두배 늘어나면 계산시간은 대략 4배 증가.

## 4.2 경사 하강법
- 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘.
- 기본적인 아이디어는 비용함수(MSE 등)을 최소화하기 위해 반복해서 파라미터를 조정.
- θ를 임의의 값으로 시작(무작위 초기화)해서 스텝(학습의 단계)마다 조금씩 비용함수가 감소되는 방향으로 진행하여 최솟값에 수렴할 때까지 점진적으로 향상.
- 가장 중요한 파라미터는 스텝의 크기로 학습률 하이퍼파라미터로 결정.
- 학습률이 너무 작으면 시간이 오래걸리고, 너무 크면 값이 이전보다 더 높은 곳으로 올라갈 수도 있음.
![image](https://user-images.githubusercontent.com/75361137/149302592-ba1e29a1-1904-4ea3-b3f8-2014b8aa5d05.png)
- 그림과 같이 지역최솟값에서 멈출수도 있음. 실제로 최솟값은 전역최솟값.
- 선형회귀를 위한 MSE 비용함수는 곡선을 가르지르지 않는 볼록함수. -> 지역최솟값은 없고 단 하나의 전역최솟값만 존재.
- 모델이 가진 파라미터가 많을 수록 공간의 차원은 커지고 검색이 어려워짐. -> 선형회귀는 차원이 굉장히 적음.

### 4.2.1 배치 경사 하강법 
- 편도함수 : 각 모델 파라미터가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산.

![image](https://user-images.githubusercontent.com/75361137/149320692-d354e9a1-1f4b-4a2f-a9c2-4bce200ccbb3.png)
- 편도함수를 각각 계산하는 대신 gradient vector로 한번에 계산 가능.

![image](https://user-images.githubusercontent.com/75361137/149320926-66b69b35-76e9-4392-bae7-19c9e109cb2f.png)
- 이 공식이 경사하강법 매 스텝마다 전체 훈련 세트 X에대해 계산. -> 배치 경사 하강법 알고리즘.
- 매 스텝마다 훈련 데이터 전체를 사용하므로, 훈련세트가 크기가 크면 느림. But 특성 수에 민감하지 않기 때문에 특성이 많은 경우 정규방정식이나 SVD 분해를 대체하여 사용.
- 다음 스텝으로 가기 위한 θ는 ![image](https://user-images.githubusercontent.com/75361137/149321622-ba8e0f35-20a0-4d6e-bf3a-749e36b5989b.png)로 구함.
- θ - 학습률 * gradient vector
- gradient vector을 빼주는 이유는 스텝이 전역 최솟값의 좌측에서 시작한 경우 기울기는 음(-)의 값을 가지므로, -와 만나 +가 되어 θ값을 키워 우측으로 이동해야 하기 때문.

![image](https://user-images.githubusercontent.com/75361137/149322129-d22856af-ad7f-4e8e-8540-01c00d987a2c.png) 
- 적절한 학습률을 찾는것이 경사하강법에서 가장 중요. 그렇기에 그리드 탐색을 사용하는데, 시간을 줄이기 위해 반복 횟수를 제한.
- 반복횟수를 지정할 때는, 우선 반복횟수를 크게 지정하고 gradient vector가 매우 작아지면, 알고리즘을 중지.

### 4.2.2 확률적 경사 하강법(SGD)
- 매 스텝에서 한 개의 샘플을 무작위로 선택하고 그 샘플에 대한 gradient vector 계산.
- 매 반복에서 다루는 데이터가 적기 때문에 속도가 매우 빠르고, 1개 샘플에 대한 메모리만 필요하므로 매우 큰 훈련 데이터 셋도 처리 가능.
- 다만 확률적이므로 비용함수가 최솟값에 다다를 때까지 요동치며 감소. 즉, 불안정함.
- 무작위성은 지역 최솟값을 건너뛰도록 도와주지만, 전역최솟값에 다다르지는 못하게 함.
- 그래서, learning rate(학습률)을 크게 설정하고 점진적으로 감소시켜 전역최솟값에 도달하게 만듬. -> 이 학습률을 결정하는 함수가 learning schedule.

### 4.2.3 미니배치 경사 하강법
- 미니배치라고 부르는 임의의 작은 샘플 세트에 대해 gradient vector을 계산.
- 행렬 연산에 최적화된 하드웨어(GPU 등)을 사용하면 성능향상을 얻을 수 있음.
- 미니배치를 어느 정도 크게하면 SGD보다 덜 불규칙하게 움직여 최솟값에 더 가까이 도달할 수 있으나, 지역최솟값에서 빠져나오기는 더 힘듬.

## *정리*
| 항목 | 정규방정식 | SVD | 배치 경사 하강법 | 확률적 경사 하강법 | 미니배치 경사 하강법 |
| ---------------- | ---------------- | --------------- | ---------------- | --------------- | ----------------|
| m(훈련샘플 수)이 클때 | 빠름 | 빠름 | 느림 | 빠름 | 빠름 |  
| 외부 메모리 학습 지원 | No | No | No | Yes | Yes | 
| n(특성 수)이 클때 | 느림 | 느림 | 빠름 | 빠름 | 빠름 | 
| 하이퍼파라미터 수 | 0 | 0 | 2 | >=2 | >=2 | 
| 스케일 조정 필요 | No | No | Yes | Yes | Yes | 
| Scikit-learn | N/A | LinearRegression | N/A | SGDRegressor | SGDRegressor | 
- 미니 배치는 SGDRegressor에서 partial_fit 메서드로 모델 파라미터를 초기화 하지 않고 반복적으로 호출할 수 있지만, 샘플을 뽑고 그 샘플을 하나씩 매치하는 것이므로 엄밀히 말하면 미니배치는 아님.


## 4.3 다항 회귀
- 비선형 데이터를 학습하기 위해 선형모델을 사용하는 기법.
- 각 특성의 거듭제곱을 새로운 특성(변수)으로 추가하고, 이 확장된 변수를 포함한 데이터셋에 선형 모델을 훈련시키는 것.
- Scikit-learn의 PolynomialFeatures를 사용해 새로운 변수 추가. ex)제곱한 변수
- PolynomialFeatures(degree=d)는 변수가 n개인 배열의 변수를 ![image](https://user-images.githubusercontent.com/75361137/149613343-8f8d952f-c56f-4dba-928b-3bea45ef3904.png)으로 변환하므로 너무 늘어나지 않도록 주의. 
- interaction_only=True 설정하면 거듭제곱이 포함된 항은 제외(즉, 일차항만 남음). get_feature_names()로 만들어진 변수의 차수를 확인가능.

## 4.4 학습 곡선
- 고차 다항 회귀를 적용할 수록, 보통의 선형 회귀보다 훨씬 더 훈련데이터에 fitting할 것.(과대적합이 일어남)

![image](https://user-images.githubusercontent.com/75361137/149613442-bcac1104-0bc6-4014-bc84-06e33de01580.png)
- 얼마나 복잡한 모델을 사용할지, 어떤 모델이 과대적합 또는 과소적합 되었는지 알 수 있는 방법은 교차 검증과 학습 곡선을 살펴보기.
- 교차 검증은 훈련데이터 성능은 좋으나 교차검증점수가 낮으면 과대적합, 둘다 좋지않으면 과소적합.
- 학습 곡선은 훈련 set과 검증 set의 모델 성능을 크기 함수로 출력. 크기가 다른 서브 세트들을 만들어 모델을 여러번 훈련 시킴.

![image](https://user-images.githubusercontent.com/75361137/149613669-8797c62b-ab0c-4f7f-a71c-110880a4af6c.png)
- 이 그림은 과소적합의 전형적인 모습. 두 곡선이 수평한 구간을 만들고 꽤 높은 오차에서 매우 가까이 근접. (학습 데이터의 1,2 size일 때 좋은 모델)
- 과소적합되어 있을 때는 훈련 샘플을 더 추가해도 소용없음. 더 복잡한 모델을 사용하거나 더 나은 변수를 사용해야함.

![image](https://user-images.githubusercontent.com/75361137/149613714-ad1a7443-5413-4680-9003-c53ba2921918.png)
- 이 그림은 과대적합 모습. 아까와 비슷해보이나 두가지 차이점 존재.
- 1. 훈련 데이터의 오차가 선형회귀 모델보다 낮음.
- 2. 두 곡선 사이에 공간 존재. 즉, 훈련데이터에서의 성능이 검증데이터 보다 훨씬 나음. -> 더 큰 훈련 세트 사용시 두 곡선이 더 가까워짐.
- 과대적합 모델을 개선하는 방법은 검증 오차가 훈련 오차에 근접할 때 까지 더 많은 훈련 데이터를 추가.

### 편향/분산 트레이드 오프
- 편향 : 잘못된 가정으로 인한 것 ex) 실제로는 2차 데이터 이나 선형으로 가정. -> 훈련 데이터에 과소적합 되기 쉬움.
- 분산 : 훈련데이터에 있는 작은 변동에 모델이 과도하게 민감하기 때문에 존재. 자유도가 높은 모델 ex)고차다항회귀모델 이 높은 분산을 가지기 쉬워 훈련 데이터에 과대적합 되는 경향이 있음.
- 줄일 수 없는 오차 : 데이터 자체에 있는 잡음 때문에 발생. 데이터에서 잡음을 제거하면 오차를 줄일 수 있음.
- 보통 모델의 복잡도가 커지면 분산이 늘어나고 편향은 줄어들음. 반대로 모델의 복잡도가 줄어들면 편향이 커지고 분산이 작아짐. -> 이것이 trade-off.

## 4.5 규제가 있는 선형 모델
- 과대적합을 감소시키는 좋은 방법은 모델을 규제하는 것.
- 다항회귀모델은 다항식의 차수를 감소시키고, 선형회귀모델은 모델의 가중치를 제한함.

### 4.5.1 릿지(Ridge) 회귀
- 선형회귀에 규제가 추가된 버전. 규제항 ![image](https://user-images.githubusercontent.com/75361137/149613951-69775b13-729f-4202-98ba-ba931f390df3.png)이 비용함수에 추가됨.
- 규제는 비용함수에 추가되는 것으로, 테스트 셋에서 성능 평가하거나 실제 샘플을 예측할 때는 적용하지않음.
- ![image](https://user-images.githubusercontent.com/75361137/149615523-6617b9c8-4015-48fc-9905-f80e7273eced.png) 릿지회귀의 비용함수.
- 하이퍼파라미터 α는 모델을 얼마나 많이 규제할지 조절. α=0이면 선형회귀와 같으며, α가 매우 크면 모든 가중치가 0에 가까워져 데이터의 평균을 지나는 수평선이 됨. -> 즉, 분산은 줄고 편향이 증가.
- 위 식에서 i가 1부터 시작하는 것은 θ0는 규제되지 않는 것을 의미.
- 이를 경사하강법에 적용하려면 MSE gradient vector에 αw를 더함.
- 릿지 회귀는 입력특성의 스케일에 민감하기에 수행 전에 데이터의 스케일을 맞추는 것이 중요(대부분의 규제 모델들은 모두 마찬가지).
- scikit-learn에서 릿지 회귀 적용하려면 penalty='ℓ2'를 사용하면 됨. -> ℓ2노름의 제곱을 2로나눈 규제항을 추가하는 것.

### 4.5.2 라쏘(Lasso) 회귀
- 릿지 회귀처럼 비용 함수에 규제항을 더하지만, 가중치 벡터의 ℓ1 노름을 사용.
- ![image](https://user-images.githubusercontent.com/75361137/149615857-aeba34a0-f83a-4c22-a958-38c772d7a478.png) 라쏘회귀의 비용함수. -> 릿지회귀와 다른 점으로는 1/2가 없다는 것과, θ의 절대값 사용.
- 중요한 특징으로는 덜 중요한 변수의 가중치를 제거하려고 함. (가중치가 0이 됨)
- 즉, 자동으로 변수를 선택하고 희소모델(sparse model)을 만듬. (0이 아닌 변수의 가중치가 적음)

![image](https://user-images.githubusercontent.com/75361137/149616032-8eae457c-e211-49ce-898c-8f637f5ff924.png)
- 왼쪽 열은 MSE 비용함수는 동일하지만, ℓ1 ℓ2 penalty 그래프에 있는 등고선은 다름.
- 오른쪽 열은 비용함수에 규제항이 포함되어 있어 등고선이 다르게 나타나며, 최적 파라미터 값(빨간 네모)이 달라진 것을 볼 수 있음.
- scikit-learn에서 라쏘 회귀 적용하려면 penalty='ℓ1'를 사용하면 됨. 

### 4.5.3 엘라스틱넷(Elastic Net)
- 엘라시틱넷은 릿지와 라쏘를 절충한 모델.
- 규제항은 릿지와 라쏘의 규제항을 단순히 더하고, 혼합 정도를 혼합비율 r을 사용해 조절. (r=0이면 릿지, r=1이면 라쏘와 같음)
- ![image](https://user-images.githubusercontent.com/75361137/149616140-f3138831-c05b-46de-8cda-38c3ae5543c6.png) 엘라스틱넷 비용함수.
- scikit-learn에서 엘라스틱넷 적용하려면 ElasticNet을 import하여 l1_ratio=r로 조절하여 사용. 

## *정리*
- 언제 이 모델들을 사용하는 것이 좋은가?
- 규제가 있는 것이 없는 것보다 일반적으로 좋음. (릿지, 라쏘, 엘라스틱넷)
- 릿지가 일반적으로 기본이 되지만, 쓰이는 변수가 몇 개뿐 즉, 적으면 라쏘나 엘라스틱넷이 나음.
- 라쏘와 엘라스틱넷은 덜 중요한 변수의 가중치를 0으로 만들기 때문에 변수 선택의 효과가 있기 때문.
- 또한 변수의 수가 훈련샘플의 수보다도 많고, 변수 몇개가 강하게 연관(다중공선성)되어 있을 경우에는 엘라스틱넷을 선호.

### 4.5.4 조기 종료(early stopping)
- 위의 규제말고 다른 방식으로는 검증에러가 최솟값에 도달하면 훈련을 중지하는 조기종료 방식.
- 훈련 시 에포크가 진행됨에 따라 알고리즘이 점차 학습되어 예측 에러가 줄어드는데, 감소하던 에러가 다시 상승하면 과대적합되기 시작하는 것을 의미.
- Early Stopping은 검증에러가 최소에 도달하는 즉시 훈련을 멈추는 것.
- 확률적 경사 하강법(SGD)이나 미니배치 경사하강법은 비용함수의 곡선이 매끄럽지 않아 최솟값에 도달했는지 확인 어려움. -> 검증오차가 일정시간 동안 최솟값보다 클 때 학습을 멈추고 최소로 돌아가는 방식 사용.

## 4.6 로지스틱 회귀(Logistic Regression)
- 로지스틱 회귀는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용. 즉, 분류문제에서 주로 사용.
- 다른 선형 모델처럼 ℓ1,ℓ2 페널티를 사용하여 규제할 수 있고, ℓ2가 기본.
- Scikit-learn의 로지스틱 회귀 모델의 규제 강도를 조절하는 하이퍼파라미터는 일반적으로 사용하는 alpha가 아니라 그 역수에 해당하는 C. C가 클수록 모델의 규제가 줄어듬.

### 4.6.1 확률 추정
- 선형 회귀 모델과 마찬가지로 입력 변수의 가중치 합을 계산. 결과값의 로지스틱을 출력.
- ![image](https://user-images.githubusercontent.com/75361137/149628570-5f8b6f9f-a0fe-45ba-827c-f205d3a7228a.png) 로지스틱 회귀 모델의 확률 추정식.
- ![image](https://user-images.githubusercontent.com/75361137/149628602-9936e4f7-8451-4952-a8aa-7057367b62dc.png) 로지스틱 σ(·)은 0~1 사이의 값을 출력하는 시그모이드 함수.
- ![image](https://user-images.githubusercontent.com/75361137/149628649-924a6010-ab8f-46a1-9e66-36839239a198.png) 
- S자 형태를 띄며, 어떤 값을 대입하면 0~1 사이의 값을 반환.
- 샘플 x가 양성 클래스에 속할 확률 p̂을 추정해보면 ![image](https://user-images.githubusercontent.com/75361137/149628797-d03ee5d7-9467-4ba7-95bb-9587bac95020.png)
- 즉, t < 0이면 σ(t) < 0.5이고, t ≥ 0이면 σ(t) ≥ 0.5이므로, θ^T·x가 양수일때 1(양성 범주), 음수일 때 0(음성 범주)이라고 예측.
- Scikit-learn에서는 predict() 메서드로 양성인지 음성인지 결과 반환, predict_proba()로 시그모이드 함수를 적용해 계산한 확률 반환.

### 4.6.2 훈련과 비용 함수
- 훈련의 목적은 양성 샘플(y=1)에 대해서는 높은 확률을 추정하고 음성 샘플(y=0)에 대해서는 낮은 확률을 추정하는 모델의 파라미터 벡터 θ를 찾는 것.
- ![image](https://user-images.githubusercontent.com/75361137/149628955-08c0f1e9-2060-40ba-8a2c-dc1d07d47a54.png) 하나의 훈련샘플 X에 대해 나타낸 비용함수 식.

![image](https://user-images.githubusercontent.com/75361137/149629108-32a56303-497d-4409-bf14-ba07e5d69343.png)

- 위 식을 직관적으로 이해하면, y=1일때 t가 0에 가까워 질수록 -log(t)가 매우 커져서 비용이 크게 증가.
- 즉, 양성 샘플(y=1)인데 이 확률을 0에 가깝게 추정(음성이라고 추정)하면 비용함수가 매우 커진다는 의미.
- 반대로 t가 1에 가까워지면 -log(t)는 0에 가까워져 비용은 0.
- 전체 훈련세트에 대한 비용함수는 모든 훈련 샘플의 비용 평균한 것. -> 이를 로그 손실(log loss)라고 함.

![image](https://user-images.githubusercontent.com/75361137/149629300-c921b3e3-28b7-455f-8adf-5da1ec5519c2.png)
- 대괄호[] 안에 보면 양성 범주 일때(y=1) y(i)=1이 되고 1-y(i)=0이 되어 첫번째 항만 남게 되고, 반대의 경우는 두번째 항만 남으면서 결국 하나의 샘플에 대한 비용함수 C(θ)와 같아짐.
- 이 비용함수의 최솟값을 계산하는 해는 존재하지 않지만, 볼록함수이므로 경사하강법 등이 전역 최솟값을 찾음.

![image](https://user-images.githubusercontent.com/75361137/149629391-01181a5e-ab78-49e2-bfd4-4ad566ccfcfc.png)
- j번째 모델 파라미터 θj에 대해 편미분하면, 비용함수 MSE의 편도함수와 매우 비슷한 형태.
- 모든 편도함수를 포함한 gradient vector을 만들면 배치 경사하강법을 사용가능.

### 4.6.3 결정 경계
![image](https://user-images.githubusercontent.com/75361137/149629528-7d25d4e8-b027-4873-b761-cc0c1737b99a.png)
- Iris-Verginica와 아닌것의 중첩되는 구간이 존재.
- 양쪽의 확률이 50%가 되는 1.6cm근방에서 결정경계가 만들어지고, 분류기는 이보다 크면 Iris-Verginica, 작으면 아니라고 예측.

![image](https://user-images.githubusercontent.com/75361137/149629645-90ab6b43-0c17-4251-967a-a64993359c96.png)
- 같은 데이터 셋을 꽃잎 너비와 꽃잎 길이 두 개의 특성으로 보여줌.
- 점선은 모델이 50% 확률을 추정하는 지점, 이 모델의 결정경계.
- (x1,x2)의 조합에 따라 Iris-Verginica가 속할 확률을 15%부터 90%까지 나타냄.

### 4.6.4 소프트맥스 회귀
- 로지스틱 회귀 모델은 여러 개의 이진분류기를 훈련시켜 연결하는게 아니라 직접 다중 클래스를 지원함. -> 소프트맥스 회귀(Softmax Regression) 또는 다항 로지스틱 회귀(Multinomial Logistic Regression)
- 참고로 소프트맥스 회귀 분류기는 한번에 하나의 클래스만 예측(다중 범주이지, 다중 출력이 아님). 따라서 Iris데이터 처럼 상호 배타적인 범주에 대해서 사용 가능(하나의 사진에 여러 사람 인식은 불가능).
- 샘플 x가 주어지면 소프트맥스 회귀모델이 각 클래스 k에 대한 점수 s_k(x)를 계산하고, 그 점수에 소프트맥스 함수를 적용해 각 클래스의 확률 추정. ![image](https://user-images.githubusercontent.com/75361137/149629909-491408f6-ff41-41ff-85a6-1f8c52984946.png)

![image](https://user-images.githubusercontent.com/75361137/149629969-4f9ac2d5-575c-4d1f-acf6-3db02e2fd9a9.png)
- 소프트맥스 함수. k : 범주의 수, s(x) : 샘플 x에 대한 각 범주의 점수를 담고 있는 vector, σ(s(x))k : 이 샘플이 범주 k에 속할 확률
- 소프트맥스 회귀 분류기도 로지스틱 회귀와 마찬가지로 추정확률이 가장 높은 클래스를 선택.

![image](https://user-images.githubusercontent.com/75361137/149630028-b5a02457-5b3a-4957-96c0-7ba362af9977.png)
- argmax연산은 함수를 최대화하는 변수의 값을 반환. 이 식에서는 추정확률 σ(s(x))k가 최대인 k값을 반환.
- 모델 훈련의 목적은 모델이 타깃범주에 대해서는 높은 확률을 추정하도록 만드는 것.
- 크로스 엔트로피(cross entropy) 비용함수를 최소화 하는 것은 타깃 클래스에 대해 낮은 확률을 예측하는 모델을 억제.
- 크로스 엔트로피는 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는지 측정하는 용도로 사용.
- 크로스 엔트로피 비용함수와 gradient vector을 계산할 수 있으므로 비용함수를 최소화 하기 위한 파라미터 행렬 Θ를 찾기 위해 경사하강법 사용.
- Scikit-learn의 LogisticRegression은 범주가 2이상이면 OvA(일대다) 전략을 사용. 
- multi_class='multinomial' 옵션과 solver='lbfgs' 옵션으로 소프트맥스 회귀를 지원하도록 알고리즘을 지정.
- 하이퍼파라미터 C를 사용해 조절할 수 있는 ℓ2규제가 적용.

![image](https://user-images.githubusercontent.com/75361137/149630450-809c9cf0-614a-45d2-8f7e-078f28f2124e.png)
- 결정경계를 배경색으로 구분하여 나타내었으며 클래스 사이의 결정경계가 모두 선형.
- Iris-Versicolor에 대한 확률을 곡선으로 나타냄. 

## 개요
- 결정 트리(Decision Tree)는 분류와 회귀작업 그리고 다중출력 작업도 가능하며, 매우 복잡한 데이터셋도 학습 가능한 강력하고 다재다능 머신러닝 알고리즘.
- 이는 랜덤 포레스트(Random Forest)의 기본 구성 요소, 즉 결정트리의 집합이 랜덤포레스트.

## 6.1 결정 트리 학습과 시각화
- Scikit-learn의 DecisionTreeClassifier 모듈을 사용하여 학습.
- export_graphviz() 함수를 사용해 그래프 정의를 파일로 출력하여 결정트리를 시각화.

![image](https://user-images.githubusercontent.com/75361137/149957343-ce55e0dc-ea81-4b39-8cd7-f034e628eb64.png)

## 6.2 예측하기
- 위의 그림을 보면 결정 트리 분류기(DecisionTreeClassifier)를 사용한 결과로, 변수로 사용한 꽃잎 길이와 꽃잎 너비의 조건에 따라 분류.
- sample은 적용된 훈련샘플 수(개수), value는 각 클래스의 훈련샘플 수(개수), gini는 불순도를 측정. ![image](https://user-images.githubusercontent.com/75361137/149962849-b3a69be9-f314-431a-805b-cddf5e0e5738.png)
- Pi는 i번째 노드에 있는 훈련 샘플 중 범주 k에 속한 샘플의 비율.
- 한 노드의 모든 샘플이 같은 클래스에 속해 있다면 이 노드를 순수(gini=0)하다고 함. -> gini가 높을 수록 불순도가 높다는 의미.
- Scikit-learn은 이진 트리만 만드는 CART 알고리즘 사용. -> 리프 노드 외의 노드는 모두 자식 노드를 두 개씩만 가짐.

![image](https://user-images.githubusercontent.com/75361137/149963271-153f2776-8a44-4d1b-972c-2dd8096262be.png)
- max_depth=2로 설정하였기 때문에 두번 나눠짐. (실선, 파선) -> 길이 2.45 이상이하, 너비 1.75 이상이하
- max_depth=3으로 설정하면 결정경계를 추가로 만듬. (점선) -> max_depth 값을 크게 잡으면 과대적합 위험도 커짐.

###*모델 해석*
- 결정 트리 처럼 직관적이고 결정 방식을 이해하기 쉬운 모델을 화이트박스 모델이라고 함.
- 랜덤 포레스트나 신경망 같은 왜 그런 예측을 만드는지 쉽게 설명하기 어려운 모델을 블랙박스 모델이라고 함. -> 성능은 뛰어나며 예측 만드는 연산과정은 쉽게 확인 가능.
- 신경망 같은 모델이 어떤 요소가 예측에 영향을 주었는지 알기 어려운 반면에, 결정 트리는 간단하고 명확한 분류 방법을 사용하여 어떤 요소인지 파악 가능.

## 6.3 클래스 확률 추정
- 한 샘플이 특정 클래스 k에 속할 확률을 추정 가능.
- 이 샘플이 속하는 리프노드를 탐색하고, 해당 노드의 클래스 확률을 반환.
- Scikit-learn에서는 predict_proba를 사용하여 확률을 반환.

## 6.4 CART 훈련 알고리즘
- Scikit-learn은 결정 트리를 훈련시키기 위해 CART(Classification and Regression Tree)알고리즘을 사용.
- 먼저 훈련 세트를 하나의 변수 k의 임곗값 tk를 사용해 가장 순수한 두개의 서브셋으로 나눔.
- 같은 방식으로 서브셋을 또 서브셋으로 나누기 위한 변수 k와 임곗값 tk를 찾는 과정을 반복.
- max_depth(최대 깊이)가 되거나, 불순도를 줄이는 분할을 찾을 수 없을 때 중단.
- min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes 의 매개변수 또한 중지 조건에 관여.

![image](https://user-images.githubusercontent.com/75361137/149965406-b2f01dc2-fc8a-4bbd-8fb2-313c802942a6.png)
- 탐욕적 알고리즘을 활용한 이 비용함수를 최소화 하는 특성 k와 임곗값 tk를 결정.
- 최적의 트리를 찾는 것은 NP-완전(불가능) 문제. -> 시간이 매우 많이 필요하고 매우 작은 훈련 세트에 적용하기 어려우므로, 납득할 만한 좋은 솔루션으로 만족해야함.

## 6.5 계산 복잡도
- 예측을 위해서는 루트 노드에서 리프 노드까지 탐색해야 하는데, 트리는 일반적으로 균형을 이루고 있음.
- 각 노드가 하나의 변수값만 확인하기 때문에 변수와 무관하게 log2(m)개의 노드를 거치므로 훈련 세트가 커도 예측속도가 매우 빠름(𝑂(log 𝑚))
- 훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 변수를 비교하므로, 훈련복잡도는 𝑂(𝑛 ⋅ 𝑚 ⋅ log(𝑚)). (m은 훈련데이터 수, n은 변수의 수)
- 훈련 세트가 작을 경우 presort=True를 지정하면 미리 데이터를 정렬해 속도를 개선할 수 있음. 또한 규제를 추가하는 방법도 있음. 훈련세트가 크면 속도가 많이 느려짐.

## 6.6 지니 불순도 또는 엔트로피?
- 기본적으로 DecisionTreeClassifier의 default 매개변수는 gini이지만 criterion='entropy'로 지정하여 엔트로피 불순도를 사용 가능. cf) DecisionTreeRegressor의 Default는 mse.
- 엔트로피는 분자의 무질서함을 측정하는 것으로, gini와 마찬가지로 순수한 샘플만이 존재할 때 0.

![image](https://user-images.githubusercontent.com/75361137/149967511-7c3e0cc3-5216-45e3-ad39-15ed508f1543.png)
- gini와 entropy중 어떤 것을 사용할지는 gini가 기본적으로 계산이 빠르기 때문에 기본값으로 좋음.
- 그러나 gini는 가장 빈도 높은 클래스를 분할하고 나머지를 고립시키는 경향이 있는 반면, entropy는 균형 잡힌 트리를 만듬.

## 6.7 규제 매개변수
- 결정트리는 훈련데이터에 대한 제약사항이 거의 없기에, 규제(결정 트리의 자유도를 제한)를 주지 않으면 훈련 데이터에 아주 가깝게 맞추려고 하기 때문에 과대적합 되기 쉬움. cf)선형 모델은 데이터가 선형일 거라 가정.
- 결정 트리는 훈련되기 전에 파라미터 수가 결정되지 않기에 비파라미터 모델로 모델 구조가 자유로움. cf)선형 모델같은 파라미터 모델은 정의된 파라미터 수를 가지므로 자유도 제한.

| 매개변수 | 설명 | 
| ---------------- | ---------------- | 
| max_depth | default값이 None이므로 최대 깊이까지 학습하기에 과대적합 가능성이 있으므로 제한을 주어야함. |
| min_samples_split | 분할되기 위해 노드가 가져야 하는 최소 샘플 수 |
| min_samples_leaf | 리프 노드가 가지고 있어야 할 최소 샘플 수 |
| min_weight_fraction_leaf | min_samples_leaf와 같지만 가중치가 부여된 전체 샘플 수에서의 비율 |
| max_leaf_nodes | 리프 노드의 최대 수 |
| max_features | 각 노드에서 분할에 사용할 특성의 최대 수 |

- 이렇듯 min_매개변수를 증가시키거나, max_매개변수를 감소시키면 모델의 규제가 커짐.

![image](https://user-images.githubusercontent.com/75361137/149969078-2f312d46-0ee6-4214-9c5a-ebeb6bc2095e.png)
- 왼쪽은 규제 없이 훈련시켜 과대적합 되어보이고, 오른쪽은 규제를 주어 일반화 성능이 좋아보임.  
- 규제를 주지 않고 우선 결정트리를 훈련시킨 후 불필요한 노드를 가지치기(pruning)하는 알고리즘도 존재. (Scikit-learn에서는 사전에 미리 제한하는 pre-pruning만 지원) -> 0.22 버전에서는 비용 복잡도 기반의 사후 가지치기를 위한 ccp_alpha 매개변수가 결정트리 및 트리 기반 앙상블 모델에 추가.

## 6.8 회귀
- 회귀문제에도 결정 트리를 DecisionTreeRegressor로 사용 가능.

![image](https://user-images.githubusercontent.com/75361137/149970757-d8438e23-6da2-48c2-a678-37e307af432f.png)
- 분류와의 차이는 각 노드에서 클래스(범주)를 예측하는 것이 아니라, 어떤 값(value)를 예측.
- 해당 리프 노드의 훈련 샘플(samples)의 평균 타깃값(y) 즉, value가 예측값. 

![image](https://user-images.githubusercontent.com/75361137/149971372-d0b2b27c-d0e0-4a82-84ce-d4b502e191a2.png)
- max_depth를 3으로 늘렸을 때 영역 분할이 더 늘어남. -> 알고리즘은 예측값과 가능한 한 많은 샘플이 있도록 영역을 분할.
- 회귀 문제에서 CART알고리즘은 불순도를 최소화 하는 대신에 평균제곱오차(MSE)를 최소화 하도록 분할.

![image](https://user-images.githubusercontent.com/75361137/149972085-d33c23cb-ee70-4c4e-ad56-e757e68b010b.png)
- 마찬가지로 규제가 없다면 훈련 세트에 매우 과대적합 될 것이고, 규제를 준다면 훨씬 그럴싸한 모델이 생성.

![image](https://user-images.githubusercontent.com/75361137/149972317-217596f3-2550-4c33-8f1d-586be50b0bec.png)

## 6.9 불안정성
- 결정 트리는 이해하고 해석하기 쉬우며, 사용하기 편하고, 여러 용도로 사용하며, 성능이 뛰어난 장점을 지님.
- 그러나 단점으로는 계단 모양의 결정경게를 생성하므로 훈련 데이터셋의 회전에 매우 민감.

![image](https://user-images.githubusercontent.com/75361137/149972646-aad38c2a-85c7-43a8-be12-041c487b06f8.png)
- 왼쪽은 쉽게 구분하나, 45도 회전한 오른쪽 그림은 일반화가 어려워 보임. -> 이를 해결하기 위해 PCA를 사용(후에 설명)
- 또한 가장 큰 문제는 훈련 데이터셋의 작은 변화에 매우 민감.

![image](https://user-images.githubusercontent.com/75361137/149972939-bb62fc1e-88f1-4d4c-b86a-76cec1a33a60.png)
- 한개의 데이터 샘플을 제거하였을 뿐인데, 매우 다른 모습의 결정 트리를 보여줌.
- 이러한 불안정성 극복을 위해 랜덤포레스트(RandomForest)는 많은 트리에서 만든 예측을 평균하여 극복함.

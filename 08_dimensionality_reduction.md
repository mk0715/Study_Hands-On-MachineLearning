## 개요
- 많은 경우 훈련 샘플이 수천~수백만 개의 변수를 가지고 있어 훈련을 느리게 할 뿐 아니라, 좋은 솔루션을 찾기 힘듬. -> 차원의 저주
- 이 문제들은 변수의 수를 크게 줄여 불가능한 문제를 가능하게 만듬.
- 차원을 축소시키면 일부 정보가 유실되기 때문에 훈련속도는 빨라지나 시스템의 성능이 나빠질 수 있고, 유지관리가 어려움.
- 어떤 경우에는 잡음이나 불필요한 세부사항을 걸러내어 성능을 높일 수 있음.
- 또한 차원을 줄이면 고차원 훈련세트를 하나의 압축된 그래프로 시각화 할 수 있으며, 군집 등 시각적인 패턴을 감지해 중요한 통찰을 얻을 때도 있음.

## 8.1 차원의 저주
![image](https://user-images.githubusercontent.com/75361137/151217199-c1b2a3ea-4e6f-4b2e-b08c-c19df8831294.png)
- 고차원은 많은 공간을 가지고 있기 때문에 고차원 데이터셋은 매우 희박할 위험이 존재 -> 대부분의 훈련 데이터가 서로 멀리 떨어져 있음.
- 새로운 샘플 또한 훈련 샘플과 멀리 떨어져 있을 가능성이 높음.
- 이 경우 예측을 위해 훨씬 많은 외삽, 즉 관찰이 어려운 데이터에 대해 추측하는 것이 필요하기 때문에 저차원일 때보다 불안정.
- 간단히 말해서 훈련 세트의 차원이 클수록 과대적합 위험이 커짐.
- 이를 해결하기 위해 훈련 샘플의 밀도가 충분히 높아질 때까지 훈련 세트의 크기를 키우는 방법 존재 -> 일정 밀도에 도달하기 위해 필요한 훈련 샘플 수가 기하급수적으로 늘어나서 사실상 불가능.

## 8.2 차원 축소를 위한 접근 방법
- 투영과 매니폴드 학습이 주요한 접근법.

### 8.2.1 투영
- 대부분 문제는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않고, 많은 변수들은 변화가 없지만 그 외 변수들은 서로 강하게 연관되어 있음.
- 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 놓여있거나 가까이 있음.

![image](https://user-images.githubusercontent.com/75361137/151217821-5bc4845f-6232-4b9b-b7a8-53cd685136dc.png)
- 모든 훈련 샘플이 거의 평면 형태로 놓여있음 -> 모든 훈련 샘플을 샘플과 평면 사이의 가장 짧은 직선을 따라 투영하면 2D 데이터 셋으로 구성 가능 -> 새로운 변수에 대응.

![image](https://user-images.githubusercontent.com/75361137/151218075-5f966739-6ae4-4b1a-bbfe-e9e6592d6ae7.png)
- 그러나 많은 경우, 스위스 롤 데이터셋처럼 부분 공간이 뒤틀리거나 휘어있기도 함 -> 투영이 꼭 좋은 방법은 아님.

![image](https://user-images.githubusercontent.com/75361137/151218264-89ba5d24-2575-4993-83d5-e6780ab8718e.png)
- 이 데이터를 투영시키면

![image](https://user-images.githubusercontent.com/75361137/151218332-7b52a30c-6c34-4c8f-939b-83b1ea0cfebb.png)
- 이러한 평태의 뭉개지는 층(왼쪽)을 볼 수 있음. 원하는 스위스 롤은 오른쪽.

### 8.2.2 매니폴드 학습
- 매니폴드란 스위스 롤처럼 고차원 공간에서 휘어지거나 뒤틀린 모양을 의미.
- 많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 매니폴드를 모델링하는 식으로 작동 -> 매니폴드 학습
- 대부분 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 가정 -> 매니폴드 가정으로 근거.
- 매니폴드 가정으로 보통 처리해야 할 작업(분류, 회귀 등)이 저차원의 매니폴드 공간에 표현되면 더 간단해질 것이라는 가정을 병행.

![image](https://user-images.githubusercontent.com/75361137/151219402-00a3e833-72a6-454e-bbee-4edab570d3db.png)
- 첫번째 행 왼쪽 그림은 스위스 롤이 두개의 클래스로 나뉘어져 결정경계가 복잡하나, 오른쪽 그림은 매니폴드 공간에서 결정경계가 단순한 직선.
- 두번째 행 왼쪽 그림에서는 x=5에서 결정경계가 단순히 놓여있으나, 오른쪽 그림의 펼쳐진 매니폴드에서는 더 복잡해짐 -> 꼭 유효하지는 않은 가정.

- 즉, 모델을 훈련시키기 전에 훈련 세트의 차원을 감소시키면 속도는 빨라지나, 항상 더 낫거나 간단한 솔루션이 되지는 않음 -> 데이터셋에 달려있음.


## 8.3 PCA
- 주성분 분석(Principal Component Analysis, PCA)는 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영시키는 방식.

### 8.3.1 분산 보존
- 저차원의 초평면에 훈련 세트를 투영하기 전에 올바른 초평면을 선택.

![image](https://user-images.githubusercontent.com/75361137/151296291-eb1079b6-58c5-4455-9967-2863c7ea285d.png)
- 오른쪽 그래프는 각 축에 투영된 결과인데, 실선은 분산을 최대로 보존하며, 점선은 분산을 매우 적게 유지하고, 파선은 중간 정도로 분산을 유지.
- 분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적 -> 원본 데이터셋과 투영된 것 사이의 평균 제곱 거리를 최소화하는 축 선택.

### 8.3.2 주성분
- PCA는 훈련 세트에서 분산이 최대인 축을 찾음 -> 실선
- 첫번째 축에 직교하고 남은 분산을 최대로 보존하는 두번째 축을 찾음 -> 점선
- 고차원 데이터셋이라면 이전의 두 축에 직교하는 세번째 축을 찾으며, 데이터 셋에 있는 차원의 수 만큼 n번째 축을 찾음 -> n번째 축을 이 데이터의 n번째 주성분(PC).
- 그림에서 첫번째 PC는 벡터 c1이 놓인 축, 두번째 PC는 벡터 c2가 놓인 축, 세번째 PC는 이 평면에 수직.
- 특잇값 분해(Singular Value Decomposition, SVD)라는 표준 행렬 분해 기술로, 훈련 세트의 주성분을 찾음 -> 훈련 세트 행렬을 세 개 행렬의 곱셈으로 분해

![image](https://user-images.githubusercontent.com/75361137/151320390-6471c753-0cc0-4d98-a1df-4cc6cfbac2a9.png)
- numpy의 svd() 함수를 사용해 훈련 세트의 모든 주성분을 구할 수 있음.
- PCA는 데이터셋의 평균이 0이라고 가정하기 때문에 Scikit-learn에서는 구현이 되어 있으나, 직접 구현할때는 데이터를 원점에 먼저 맞추기.

### 8.3.3 d차원으로 투영하기
- 주성분을 모두 추출한 후, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소 가능 -> 이 초평면은 분산을 가능한 한 최대로 보존.
- d차원으로 축소된 데이터셋을 얻기 위해서는 ![image](https://user-images.githubusercontent.com/75361137/151321177-b36a53fc-8eb5-4d6e-88fc-46509c26bc81.png)
- python으로는 dot을 사용하여 투영.

### 8.3.4 사이킷런 사용하기
- Scikit-learn의 PCA모델은 SVD 분해 방법을 이용해 구현. 
- n_components로 주 성분 개수 지정.
- components_ 변수에 Wd의 전치가 담김.

### 8.3.5 설명된 분산의 비율
- 설명된 분산의 비율 즉, 각 주성분의 축을 따라 있는 데이터셋의 분산 비율은 explained_variance_ratio_ 변수에 저장.
- array[A,B]의 결과라면 -> 데이터셋 분산의 A%가 첫번째 PC를 따라 놓여있고, B%가 두번째 PC를 따라 놓여있음. 

### 8.3.6 적절한 차원 수 선택하기
- 축소할 차원 수를 임의로 정하는 것보다 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 간단 -> n_components에 주성분의 수 지정하지말고 분산의 비율을 0~1 사이로 설정.
- 데이터 시각화를 위해 차원 축소할 때는 2,3개로 줄이는 것이 일반적.
- 다른 방법으로는 설명된 분산을 차원 수에 대한 함수로 그리기.(cumsum을 그래프로 그림)

![image](https://user-images.githubusercontent.com/75361137/151324548-bd0760f9-fb87-47e4-9a5f-0e7d436a5394.png)

### 8.3.7 압축을 위한 PCA
- 차원축소 후에는 훈련 세트의 크기가 줄어들며, 이러한 크기 축소는 SVM같은 분류 알고리즘의 속도를 높일 수 있음.
- 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용하여 되돌릴 수도 있으나(inverse_transform), 일정량의 정보를 잃어버렸기 때문에 완전히 동일하지는 않음.

![image](https://user-images.githubusercontent.com/75361137/151325259-d6f050b3-37c8-4c27-9268-5138dbe4c1fa.png)

- 재구성 오차(Reconstruction error)은 원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리.

### 8.3.8 랜덤 PCA
- 확률적 알고리즘을 사용해 처음 d개의 주성분에 대한 근삿값을 빠르게 찾을 수 있음 -> svd_solver 매개변수를 'randomized'로 지정.
- Default값은 auto -> m 또는 n이 500보다 크고, d가 m 또는 n의 80%보다 작으면 자동으로 랜덤 PCA 사용. 완전 SVD 방식을 쓰려면 'full'로 지정.

### 8.3.9 점진적 PCA
- PCA는 훈련 세트가 크면 메모리에서 받아들일 수 없는 문제점이 존재 -> 점진적 PCA(Incremental PCA, IPCA)로 미니배치로 나누어 훈련세트가 커도 유용하게 만듬.
- Scikit-learn에서는 IncrementalPCA를 사용하고, numpy의 memmap 클래스를 사용할 수도 있음.

## 8.4 커널 PCA
- 커널 트릭(비선형 분류와 회귀 가능하게 도와주는)을 사용해 차원 축소를 위한 복잡한 비선형 투형 수행 가능.
- Scikit-learn의 KernelPCA를 사용해 RBF커널로 적용.

![image](https://user-images.githubusercontent.com/75361137/151330220-809cfb7f-6055-4e09-9955-7bc13bb332e8.png)
- 스위스 롤을 2차원으로 축소 시킴.

### 8.4.1 커널 선택과 하이퍼파라미터 튜닝
- 커널 PCA는 비지도 학습이므로 커널과 하이퍼파라미터의 선택 기준이 없음 -> 차원 축소는 지도학습의 전처리 단계로 활용되므로 그리드 탐색을 사용해 선택 가능.
- 차원 축소를 먼저 시행 -> 가장 높은 정확도를 위해 커널과 파라미터 설정해서 그리드 탐색 등으로 선택.
- 또 다른 방법으로는 재구성. -> 완전한 비지도 학습 방법으로 가장 낮은 재구성 오차를 만드는 커널과 하이퍼파라미터 선택.

![image](https://user-images.githubusercontent.com/75361137/151334633-2765b28b-8f5d-4601-a8d9-454556d55f31.png)
- 차원 축소된 데이터를 복원하면 원본 공간이 아니라 변수 공간에 놓임. -> 무한 차원이기에 포인트와 실제 에러를 계산 불가.
- 재구성 원상으로 재구성된 포인트에 가깝게 매핑된 원본 공간의 포인트를 찾기 가능 -> 재구성 원상의 오차를 최소화하는 커널과 하이퍼파라미터 선택.
- Scikit-learn에서는 매개변수로 fit_inverse_transform=True로 설정하여 수행.

## 8.5 LLE
- 지역 선형 임베딩(Locally Linear Embedding, LLE)는 비선형 차원 축소 기술이지만, 투영이 아니라 매니폴드 학습.
- LLE는 각 훈련 샘플이 가장 가까운 이웃에 얼마나 선형적으로 연관되어 있는지 측정하고, 국부적 관계가 가장 잘 보존되는 저차원 표현을 찾음.
- 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는데 잘 작동.
- Scikit-learn에서는 LocallyLinearEmbedding 모듈을 사용.

![image](https://user-images.githubusercontent.com/75361137/151335732-66c539c1-a299-4cec-a251-c5c51616ac15.png)

## 8.6 다른 차원 축소 기법

#### 랜덤 투영(Random Projection)
- 말 그대로 선형적인 투영을 랜덤하게 하여 데이터를 저차원으로 투영.
- 실제 거리를 잘 보존한다는 것이 수학적으로 증명됨.
- 차원 축소 품질은 샘플 수와 목표 차원수에 따라 다르며, 초기 차원수에는 의존적이지 않음.

#### 다차원 스케일링(Multidimensional Scaling, MDS)
- 샘플 간의 거리를 보존하면서 차원 축소

#### Isomap
- 각 샘플을 가장 가까운 이웃과 연결하는 식으로 그래프 생성.
- 그 후 샘플 간의 지오데식 거리(Geodesic Distance, 두 노드 사이의 최단 경로를 이루는 노드의 수)를 유지하면서 차원 축소.

#### t-SNE(t-Distributed Stochastic Neighbor Embedding)
- 비슷한 샘플은 가까이, 비슷하지 않은 샘플은 멀리 떨어지도록 하면서 차원 축소.
- 주로 시각화에 많이 이용되며, 특히 고차원 공간에 있는 샘플의 군집을 시각화 할 때 사용.

![image](https://user-images.githubusercontent.com/75361137/151336513-97d9985c-63e0-43dc-894b-be3490066907.png)

#### 선형 판별 분석(Linear Discriminant Analysis, LDA)
- 분류 알고리즘이지만, 훈련 과정에서 클래스 사이를 가장 잘 구분하는 축을 학습.
- 학습된 축은 데이터가 투영되는 초평면을 정의하는데 사용.
- 투영을 통해 가능한 한 클래스를 멀리 떨어지게 유지시킴 -> SVM 같은 분류 알고리즘 적용 전 차원 축소에 좋음.


## 개요
- 많은 데이터는 대부분 레이블이 없음. 즉, 입력 특성 X는 있지만 레이블 y는 없음.
- 레이블을 부여하는 작업은 시간이 오래 걸리고 비용이 많이 들며 지루함.
- 레이블이 없는 데이터를 바로 사용하기 위한 것이 비지도 학습.
- 군집(Clustering), 이상치 탐지(Outlier Detection), 밀도 추정(Density Estimation) 등.

## 9.1 군집
- 비슷한 샘플을 구별해 하나의 클러스터(Cluster) 또는 비슷한 샘플의 그룹으로 할당하는 작업.

![image](https://user-images.githubusercontent.com/75361137/152196666-2bd79c6c-02fe-4213-8869-9b66c922725c.png)
- 분류 : 데이터 셋이 레이블 되어 있으며, 로지스틱 회귀, SVM, 랜덤 포레스트 등의 분류 알고리즘 사용.
- 군집 : 레이블이 없어 분류 알고리즘 사용 불가. 여러 변수를 사용하면 군집 알고리즘으로 클러스터 세 개를 구분 가능.

| 군집 사용 어플리케이션 | 설명 | 
| ---------------- | ---------------- | 
| 고객 분류 | 구매 이력이나 웹사이트 내 행동 등을 기반으로 클러스터 생성. 고객이 무엇을 원하는지 이해하는데 도움이 되므로 추천 시스템 생성. |
| 데이터 분석 | 새로운 데이터셋에 군집 알고리즘을 실행하고 각 클러스터를 분석. |
| 차원 축소 기법 | 각 클러스터에 대한 샘플의 친화성 측정하여, 각 샘플의 특성 벡터를 클러스터 친화성 벡터로 변환. 클러스터 개수로 벡터의 차원 결정. |
| 이상치 탐지 | 모든 클러스터에 친화성이 낮은 샘플은 이상치일 가능성이 높음. |
| 준지도 학습 | 일부 레이블된 샘플을 군집을 수행해 모든 샘플에 레이블 전파. |
| 검색 엔진 | 일부 검색 엔진이 제시된 이미지와 비슷한 이미지를 찾아주며, 비슷한 이미지는 동일 클러스터에 속함. |
| 이미지 분할 | 색을 기반으로 픽셀을 클러스터로 모은 후, 각 픽셀의 색을 클러스터의 평균 색으로 변환. 이미지에 있는 색상의 종류를 크게 줄이는 효과. |

- 클러스터에 대한 보편적인 정의는 없으며, 알고리즘이 다르면 다른 종류의 클러스터를 감지.

### 9.1.1 k-평균(K-means)
- 레이블이 없는 데이터셋 샘플 중, 반복 몇 번으로 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘.
- 각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당.
- Scikit-learn에서는 KMeans로 사용.

![image](https://user-images.githubusercontent.com/75361137/152199205-4af9d391-a868-4c6b-b9aa-fc4c78683b6b.png)
- 알고리즘이 찾을 클러스터 개수 k를 지정해야하는데, 일반적으로 이는 쉬운 일이 아님.

![image](https://user-images.githubusercontent.com/75361137/152199547-54594c2d-e98e-4d46-8d12-bfa0ae94af9d.png)
- 대부분 잘 할당 되었지만 클러스터의 크기가 다르기 때문에 잘 작동하지 않은 부분도 존재. -> 샘플을 클러스터에 할당할 때 센트로이드(특정 포인트를 중심으로 모인 샘플)까지 거리를 고려하는 것이 전부이기 때문.
- 위처럼 샘플을 하나의 클러스터에 할당하는 것을 하드 군집. -> 클러스터마다 샘플 점수를 부여하는 것은 소프트 군집. 점수로는 샘플과 센트로이드 사이의 거리, 유사도 점수 등 사용가능.
- 고차원 데이터셋을 소프트 군집으로 변환하여 k-차원 데이터셋으로 만드는 것은 효율적인 비선형 차원 축소 기법이 될 수 있음.

#### k-평균 알고리즘
- 알고리즘 작동방식
- 1. 레이블이나 센트로이드가 주어지지 않으면, 센트로이드를 랜덤으로 선정(무작위로 k개의 샘플을 뽑아 그 위치를 센트로이드로 설정)
- 2. 샘플에 레이블을 할당하고 센트로이드를 업데이트 하는 것을 센트로이드에 변화가 없을 때까지 반복(제한된 횟수 안에 수렴).
- 3. 최적으로 보이는 클러스터에 도달할 때까지 반복.

![image](https://user-images.githubusercontent.com/75361137/152295731-7993bae9-265a-41c4-94ae-732c0070dd05.png)

![image](https://user-images.githubusercontent.com/75361137/152295967-6c8c0115-9d12-4b69-a439-6c7781b32e5e.png)
- 적절한 솔루션으로 수렴하지 못할 경우, 센트로이드 초기화의 이유. -> 개선하여 이 위험 줄이기 가능.

#### 센트로이드 초기화 방법
- 다른 군집 알고리즘을 실행해 센트로이드 위치를 근사하게 파악하여 초기화.
- 또는 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택.
- 이너셔(Inertia) 즉, 각 샘플과 가장 가까운 센트로이드 사이의 평균 제곱 거리인 값을 성능 지표로 사용하여 최적의 솔루션 선택.

*k-평균++ 알고리즘*
- 초기화 단계를 다른 센트로이드와 거리가 먼 센트로이드를 선택하는 단계로 사용. -> 안좋은 솔루션으로 수렴할 가능성을 낮춤.
- Default값으로 KMeans 클래스는 이 초기화 방법 사용

#### k-평균 속도 개선과 미니배치 k-평균
- 불필요한 거리 계산을 피함으로써 알고리즘의 성능 향상. -> 삼각 부등식 사용(두 점 사이의 직선은 항상 짧은 거리가 됨)
- 샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지.

- 미니배치 k-평균은 전체 데이터셋을 사용해 반복하는 것이 아닌, 각 반복마다 미니배치를 사용해 센트로이드를 이동시키는 방법.
- 이는 알고리즘 속도를 3~4배 높여주며, 메모리에 들어가지 않는 대량의 데이터셋에 군집을 적용 가능.
- 속도는 빠른 장점이 있으나, 이너셔는 조금 더 나쁨(성능이 낮음).

#### 최적의 클러스터 개수 찾기
- 클러스터 k의 개수를 올바르게 지정하는 것이 중요.
- 이너셔는 k가 증가함에 따라 점점 작아지므로 k 선택시에는 좋은 성능 지표는 아님.

![image](https://user-images.githubusercontent.com/75361137/152297865-402d1814-2011-4744-a3a6-ed39ad53ec22.png)
- 그러나 이너셔의 감소 폭이 크게 줄어드는 지점을 선택할 수는 있음. -> 엘보 지점(그래프가 꺾이는 지점)
- 더 정확한 방법으로는 실루엣 점수 즉, 모든 샘플에 대한 실루엣 계수의 평균을 사용.
- 실루엣 계수는 (b-a)/max(a,b)로 구하며, a는 클러스터 내부의 평균거리, b는 가장 가까운 클러스터의 샘플까지 평균 거리.
- 실루엣 계수는 -1~1까지 바뀌며, 1에 가까우면 자신의 클러스터 안에 잘 속해있고 다른 클러스터와 멀리 떨어져 있다는 의미.
- 0에 가까우면 클러스터 경계에 위치한다는 의미이며, -1에 가까우면 이 샘플이 잘못된 클러스터에 할당되었다는 의미.

![image](https://user-images.githubusercontent.com/75361137/152298363-c01769a6-f9e9-4ba4-b842-2805b0d5395d.png)
- 실루엣 계수를 할당된 클러스터와 계수값으로 정렬하여 그래프로 나타낼 수도 있음.(실루엣 다이어그램)

![image](https://user-images.githubusercontent.com/75361137/152298604-17a4a45c-d231-4f6a-abf7-cbcfba476651.png)
- 높이는 클러스터가 포함하고 있는 샘플의 개수, 너비는 샘플의 정렬된 실루엣 계수(넓을 수록 좋음), 수직 파선은 전체 실루엣 점수.
- k=4일때 점수가 더 높으나, 비슷한 크기의 클러스터를 얻을 수 있는 k=5를 선택하는 것이 더 좋음.

### 9.1.2 k-평균의 한계
- 속도가 빠르고 확장이 용이하지만 단점 존재.
- 1. 최적의 솔루션을 구하기 위해 알고리즘을 여러 번 실행해야 함.
- 2. 클러스터 개수를 지정해야 함.
- 3. k-평균은 클러스터의 크기나 밀집도가 서로 다르거나 원형이 아닐 경우 잘 작동하지 않음.

![image](https://user-images.githubusercontent.com/75361137/152299367-9be09670-512e-4788-a2c9-28e2e4bd46c9.png)
- 위 이미지는 크기와 밀집도 방향이 다른 타원형 클러스터를 가졌기 때문에 잘 작동하지 않는 모습.
- 이런 타원형 클러스터에서는 가우시안 혼합 모델이 잘 작동.(후에 설명)
- k-평균을 실행하기 전에 입력 특성의 스케일을 맞춰주면 클러스터가 길쭉해지는 것을 막으며, 잘 구분되게 도와줌.

### 9.1.3 군집을 사용한 이미지 분할
- 이미지 분할은 이비지를 세그먼트 여러 개로 분할하는 작업.
- 시맨틱 분할은 동일한 종류의 물체에 속한 모든 픽셀은 같은 세그먼트에 할당. -> 합성곱 신경망 등의 복잡한 모델이 필요.
- 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당하는 색상 분할 진행.
- 색깔 하나를 하나의 컬러 클러스터로 만들고, 각 색상에 대해 픽셀의 컬러 클러스터의 평균 컬러를 찾음.(ex. 모든 초록색을 밝은 초록으로 바꿈)
- 긴 색상의 리스트를 원본 이미지와 동일한 크기로 바꿈.

![image](https://user-images.githubusercontent.com/75361137/152301334-8e279c33-a3ed-4d3d-ab30-bd397cee3877.png)
- k-평균이 비슷한 크기의 클러스터를 만들기 때문에, 6개 클러스터 부터 무당벌레의 빨간색이 독자적인 클러스터를 만들지 못하고 주위 색에 합쳐짐.

### 9.1.4 군집을 사용한 전처리
- 군집을 지도 학습 알고리즘 적용 전에 전처리 단계로 사용 가능.
- 군집은 데이터셋의 차원을 줄이지만(또는 늘리지만), 성능 향상이 되는 이유는 원본 보다 선형적으로 더 잘 구분할 수 있기 때문.
- 전처리 단계로 사용할 때는 클러스터 개수를 찾을 때 실루엣 분석이나 이너셔를 볼 필요 없음 -> 교차 검증에서 가장 좋은 분류 성능을 내는 값으로 판별.

### 9.1.5 군집을 사용한 준지도 학습
- 레이블이 없는 데이터가 많고, 있는 데이터는 적을 때 준지도 학습 사용.
- 훈련 세트를 k개의 클러스터로 모은 후, 각 클러스터에서 센트로이드에 가장 가까운 이미지(대표 이미지)를 찾고 수동으로 레이블 할당. -> 무작위 샘플 대신 대표 샘플에 레이블 할당이 좋음.
- 레이블을 동일한 클러스터에 있는 모든 샘플로 전파(레이블 전파). -> 전파할 때 전체적으로 전파하기 보다는 부분적으로 전파하는 것이 좋음(클러스터 경계에 위치한 샘플 때문)

#### *능동학습 - 불확실성 샘플링*
- 모델과 훈련 세트를 지속적으로 향상하기 위해 능동학습을 몇 번 반복 가능.
- 1. 지금까지 수집한 레이블된 샘플에서 모델을 훈련하여 레이블 되지 않은 모든 샘플에 대한 예측 생성.
- 2. 모델이 가장 불확실하게 예측한 샘플을 전문가에게 보내 레이블을 부착.(모델을 가장 크게 바꾸거나 모델의 검증점수를 바꾸는 샘플도 가능)
- 3. 레이블을 부여하는 노력만큼의 성능이 향상되지 않을 때까지 이를 반복.

### 9.1.6 DBSCAN
- 밀집된 연속적 지역을 클러스터로 정의하는 알고리즘.
- 모든 클러스터가 충분히 밀집되어 있으며, 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 내는 알고리즘.
- 작동방식 
- 1. 알고리즘이 각 샘플에서 작은 거리인 ε내에 샘플이 몇 개 놓여있는지 셈 -> 이 지역의 샘플을 ε-이웃.
- 2. ε-이웃 내에 적어도 min_samples개 샘플이 있다면 핵심 샘플로 간주.(핵심 샘플은 밀집된 지역에 있는 샘플)
- 3. 핵심 샘플의 이웃에 있는 모든 샘플은 동일한 클러스터에 속함. 이웃에는 다른 핵심 샘플이 포함될 수 있으며 핵심 샘플의 이웃의 이웃은 계속해서 하나의 클러스터를 생성.
- 4. 핵심 샘플도 아니고 이웃도 아닌 샘플은 이상치로 판단.
- Scikit-learn에서는 DBSCAN으로 사용.

![image](https://user-images.githubusercontent.com/75361137/152306544-ef35b003-4f97-4ef0-8d9c-5a9315a8d613.png)
- 왼쪽은 많은 샘플을 이상치로 판단하여서, eps(이웃 범위)를 증가시켜 오른쪽 그래프처럼 군집을 얻음.
- predict() 메소드를 지원하지 않고(새로운 샘플에 대해 클러스터 예측 불가), fit_predict() 메소드만 지원. -> 사용자가 필요한 예측기 선택.
- DBSCAN의 장점으로는 클러스터의 모양과 개수에 상관없이 감지할 수 있는 능력. 이상치에 안정적이며 하이퍼파라미터가 eps, min_samples 두개 뿐(최적의 군집개수를 자동으로 찾음). 잡음 지점도 걸러내기 때문에 이상치를 구별하기 유용.
- 단점으로는 클러스터 간의 밀집도가 크게 다르면 모든 클러스터를 잡아내는 것이 불가능. 알고리즘 시행마다 다른 결과 도출(데이터 포인트 처리 순서가 다름). 데이터의 차원이 높아질수록 eps를 지정하기 어려움.

### 9.1.7 다른 군집 알고리즘
- Scikit-learn에 여러 군집 알고리즘 구현.

#### 병합 군집
- 인접한 클러스터 쌍을 연결.
- 병합된 클러스터 쌍을 트리로 모두 그리면 클러스터의 이진 트리 생성. -> 트리의 leaf는 개별 샘플
- 병합 군집은 대규모 샘플과 클러스터에 잘 확장되며 다양한 형태의 클러스터 감지.
- 특정 클러스터 개수를 선택하는 데 도움이 되는 유용한 클러스터 트리 생성.

#### BIRCH
- 대규모 데이터셋을 위해 고안.
- 특성 개수가 너무 많지 않으면(20개 이하) 배치 k-평균보다 빠르고 비슷한 결과 생성.
- 훈련 과정에서 새로운 샘플을 클러스터에 빠르게 할당할 수 있는 정보를 담은 트리 구조 생성.
- 트리에 모든 샘플을 저장하지 않으며, 제한된 메모리를 사용해 대용량 데이터 셋 다룸.

#### 평균-이동
- 각 샘플을 중심으로 하는 원을 그린 후, 원마다 안에 포함된 모든 샘플의 평균 구하고 원의 중심을 평균점으로 이동.
- 모든 원이 움직이지 않을 때까지 평균-이동을 반복. -> 평균-이동은 지역의 최대밀도를 찾을 때까지 높은 쪽으로 원을 이동.
- DBSCAN과 유사하게 모양이나 개수에 상관없이 클러스터 색출 가능. 적은 하이퍼파라미터(Bandwidth라는 원 반경 한개). 국부적인 밀집도 추정에 의존.
- DBSCAN과 달리 내부 밀집도가 불균형할 때 여러 개로 나누는 경향. 대규모 데이터셋에는 적합하지 않음.

#### 유사도 전파
- 투표 방식 사용. -> 샘플은 자신을 대표할 수 있는 비슷한 샘플에 투표하며 알고리즘이 수렴하면 각 대표와 투표한 샘플이 클러스터 형성.
- 크기가 다른 여러 개의 클러스터를 감지할 수 있음. 대규모 데이터셋에는 적합하지 않음.

#### 스펙트럼 군집
- 샘플 사이의 유사도 행렬을 받아 저차원 임베딩을 생성(차원 축소). -> 저차원 공간에서 군집 알고리즘 사용.
- 스펙트럼 군집(Spectral Clustering)은 복잡한 클러스터 구조를 감지하고 그래프 컷(Graph Cut)을 찾는데 사용.(소셜 네트워크에서 친구의 클러스터 색출)
- 샘플 개수가 많거나 클러스터의 크기가 매우 다르면 잘 작동하지 않음.

## 9.2 가우시안 혼합
- 가우시안 혼합 모델(Gaussian Mixture Model, GMM)샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우시안 분포에서 생성되었다고 가정하는 확률 모델.
- 하나의 가우시안 분포에서 생성된 모든 샘플은 하나의 클러스터 형성.

![image](https://user-images.githubusercontent.com/75361137/152329117-890078dc-1b53-4add-9cc2-3cb4ecda91fa.png)
- 기댓값-최대화(Expectation-Maximization, EM) 알고리즘을 사용.
- 클러스터 파라미터를 랜덤하게 초기화하고 수렴할 때까지 두 단계를 반복. -> 샘플을 클러스터에 할당(기댓값 단계), 클러스터를 업데이트(최대화 단계) -> k-평균과 비슷.
- k-평균과 다른점으로는 EM은 소프트 클러스터 할당 사용. -> 기댓값 단계에서 각 클러스터에 속할 확률 예측, 최대화 단계에서 각 클러스터가 모든 샘플을 사용해 업데이트.
- EM도 k-평균과 마찬가지로 나쁜 솔루션으로 수렴할 수 있음. -> 여러 번 실행 후 가장 좋은 솔루션 설정.

- 가우시안 혼합 모델은 생성 모델(Generative Model). -> 모델에서 새로운 샘플 생성 가능, 모델의 밀도 추정 가능.
- score_samples()메서드로 위치의 확률 밀도 함수(PDF)의 로그 예측. -> 점수 높을수록 밀도 높음.
- GMM은 k-means보다 유연하게 다양한 데이터 셋에 잘 적용될 수 있으나(k-means의 단점 보완 가능 모델), 수행 시간이 오래 걸림.

### 9.2.1 가우시안 혼합을 사용한 이상치 탐지
- 이상치 탐지(Outlier Detection)는 보통과 많이 다른 샘플을 감지하는 작업. 보통 샘플은 정상치.
- 가우시안 혼합 모델을 이상치 탐지에 사용하는 방법은, 밀도가 낮은 지역에 있는 모든 샘플을 이상치로 간주. -> 사용할 밀도 임곗값을 설정.

![image](https://user-images.githubusercontent.com/75361137/152331130-72209590-6930-4f5c-989f-483a5ce80690.png)
- 이와 비슷한 특이치 탐지(Novelty Detection)는 이상치로 오염되지 않는 깨끗한 데이터셋에서 훈련하는 것이 이상치 탐지와 차이점.
- 가우시안 혼합 모델은 이상치를 포함해 모든 데이터에 맞추려고 함 -> 이상치가 너무 많으면 모델이 정상치를 바라보는 시각이 편향되고 일부 이상치가 정상으로 처리.
- 이런 경우 먼저 모델을 훈련하고 가장 크게 벗어난 이상치를 제거한 후, 정제된 데이터셋에서 모델을 다시 훈련.

### 9.2.2 클러스터 개수 선택하기
- k-평균에서는 이너셔나 실루엣 점수를 사용해 클러스터 개수 선택. 가우시안 혼합에서는 이런 지표 사용 불가. -> 클러스터가 타원형이거나 크기가 다를 때 불안정하기 때문.
- BIC(Bayesian Information Criterion)나 AIC(Akaike Information Criterion)와 같은 이론적 정보 기준(Theoretical Information Criterion)을 최소화하는 모델을 찾음.

![image](https://user-images.githubusercontent.com/75361137/152332973-2ca1d5e6-309d-4476-9d35-5a735add4567.png)

![image](https://user-images.githubusercontent.com/75361137/152333021-251520fe-936a-4801-ab1c-64778666cb84.png)
- BIC와 AIC는 모두 학습할 파라미터가 많은(클러스터가 많은) 모델에게 벌칙을 가하고 데이터에 잘 학습하는 모델에게 보상을 더함.
- 둘의 선택이 비슷하나, 다를 경우 BIC 선택 모델이 AIC 선택 모델 보다 간단한 경향 존재. -> 대규모 데이터셋에서는 잘 맞지 않음.

![image](https://user-images.githubusercontent.com/75361137/152334184-01b79e19-f270-4666-a6f0-07b6b8ae2707.png)
- 위의 경우, k=3에서 BIC와 AIC가 모두 가장 작으므로 최선의 선택.

#### *가능도 함수*
- 확률과 가능도는 구별 없이 사용되지만, 통계학에서 다른 의미를 가짐.
- 파라미터 0인 확률 모델이 주어질때
- 확률 : 미래 출력 x가 얼마나 그럴듯한지 설명.
- 가능도 : 출력 x를 알고 있을 때 특정 파라미터 값 0이 얼마나 그럴듯한지 설명.

### 9.2.3 베이즈 가우시안 혼합 모델
- 몰라. (후에 추가 예정)

### 9.2.4 이상치 탐지와 특이치 탐지를 위한 다른 알고리즘
- Scikit-learn에서 이상치 탐지와 특이치 탐지 전용으로 사용할 수 있는 알고리즘들.

#### PCA
- 보통 샘플의 재구성 오차와 이상치의 재구성 오차를 비교하면 일반적으로 후자가 훨씬 큼. -> 매우 효과적인 이상치 탐지 기법

#### Fast-MCD(Minimum Covariance Determinant)
- 데이터셋을 정제할 때 이상치 감지에 사용.
- 샘플이 하나의 가우시안 분포에서 생성되고 가우시안 분포에서 생성되지 않은 이상치로 데이터셋이 오염되었을 때, 파라미터(정상치를 둘러싼 타원 도형을) 추정시 이상치로 의심되는 샘플을 무시.
- 알고리즘이 타원형을 잘 추정하고 이상치를 잘 구분하게함.

#### 아이솔레이션 포레스트(IsolationForest)
- 고차원 데이터셋에서 이상치 감지를 위한 효율적인 알고리즘.
- 무작위로 성장한 결정 트리로 구성된 랜덤 포레스트 생성 후, 각 노드에서 변수를 랜덤하게 선택한 후 랜덤한 임곗값으로 데이터셋을 둘로 분리.
- 계속 반복해 모든 샘플이 다른 샘플과 격리될 때까지 진행 후, 일반적으로 다른 샘플과 멀리 떨어진 이상치를 감지.

#### LOF(Local Outlier Factor)
- 주어진 샘플 주위의 밀도와 이웃 주위의 밀도를 비교해 k개의 최근접 이웃보다 더 격리된 이상치를 감지.

#### one-class SVM
- 특이치 탐지에 맞는 알고리즘.
- 고차원 데이터셋에 잘 작동하나, 모든 SVM처럼 대규모 데이터셋에서는 어려움.
